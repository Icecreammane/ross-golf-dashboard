# SQLite Database Migration

Production-ready SQLite database migration for all JSON data with performance optimization and backward compatibility.

## üéØ Features

- ‚úÖ **Complete Migration**: Opportunities, fitness, golf, email, Twitter, revenue, analytics, decisions
- ‚úÖ **Performance Optimized**: All queries < 10ms with proper indexes
- ‚úÖ **Backward Compatible**: JSON files remain readable
- ‚úÖ **One-Command Upgrade**: `./upgrade.sh` does everything
- ‚úÖ **Comprehensive Tests**: 30+ tests covering all tables and queries
- ‚úÖ **Query Library**: Pre-built queries for all common operations
- ‚úÖ **Production Ready**: Error handling, backups, dry-run mode

## üìä Database Schema

### Tables Overview

| Table | Purpose | Key Indexes |
|-------|---------|-------------|
| `opportunities` | All revenue opportunities | score, source, type, status, detected_at |
| `workouts` | Fitness workouts | date, timestamp |
| `lifts` | Individual lift records | workout_id, name |
| `food_logs` | Nutrition tracking | date, timestamp, calories |
| `weight_logs` | Weight tracking | date |
| `golf_rounds` | Golf round scores | date, course, score, differential |
| `email_summaries` | Important emails | sender, timestamp, subject |
| `twitter_opportunities` | Twitter engagement | score, type, timestamp, followers |
| `social_posts` | Social media posts | posted_at, engagement |
| `conversions` | Revenue conversions | date, source, revenue |
| `decisions` | Decision tracking | timestamp, category, confidence |

### Performance Indexes

All queries are optimized with strategic indexes:

```sql
-- Example: Composite index for common queries
CREATE INDEX idx_opportunities_composite ON opportunities(source, status, score DESC);

-- Example: Timestamp index for date-range queries
CREATE INDEX idx_opportunities_detected_at ON opportunities(detected_at DESC);
```

## üöÄ Quick Start

### One-Command Migration

```bash
cd ~/clawd/database
./upgrade.sh
```

This will:
1. ‚úÖ Check prerequisites
2. ‚úÖ Backup JSON files
3. ‚úÖ Create database with schema
4. ‚úÖ Migrate all data
5. ‚úÖ Run comprehensive tests
6. ‚úÖ Verify performance
7. ‚úÖ Print summary

### Manual Migration

```bash
# Dry run (no changes)
python3 migrate.py --dry-run

# With backup
python3 migrate.py --backup

# Force overwrite existing database
python3 migrate.py --force

# Full migration with backup
python3 migrate.py --backup --force
```

## üìö Query Library Usage

### Basic Usage

```python
from database.query_library import Database

# Initialize database
db = Database()

# Check health
print(db.health_check())
```

### Opportunities

```python
# Get top opportunities
with db.opportunities as opp:
    top = opp.get_top_opportunities(limit=10, min_score=70)
    for o in top:
        print(f"[{o['score']}] {o['title']} - {o['source']}")

# Search opportunities
with db.opportunities as opp:
    results = opp.search('golf coaching')
    
# Get by source
with db.opportunities as opp:
    email_opps = opp.get_by_source('email')

# Get converted opportunities
with db.opportunities as opp:
    conversions = opp.get_converted(days=30)

# Get statistics
with db.opportunities as opp:
    stats = opp.get_stats()
    print(f"Total: {stats['total']}, Converted: {stats['converted']}")
```

### Fitness

```python
# Recent workouts
with db.fitness as fit:
    workouts = fit.get_recent_workouts(days=7)
    for w in workouts:
        print(f"{w['date']}: {w['total_lifts']} lifts")

# Workout details with lifts
with db.fitness as fit:
    workout = fit.get_workout_details(workout_id=1)
    for lift in workout['lifts']:
        print(f"{lift['name']}: {lift['weight']}lbs x {lift['reps']} x {lift['sets']}")

# Lift progress tracking
with db.fitness as fit:
    progress = fit.get_lift_progress('Bench Press', limit=10)

# Daily nutrition
with db.fitness as fit:
    today = fit.get_daily_nutrition()
    print(f"Calories: {today['total_calories']}")
    print(f"Protein: {today['total_protein']}g")

# Nutrition trend
with db.fitness as fit:
    week = fit.get_nutrition_week(days=7)

# Weight trend
with db.fitness as fit:
    weights = fit.get_weight_trend(days=30)
```

### Golf

```python
# Recent rounds
with db.golf as golf:
    rounds = golf.get_recent_rounds(limit=10)

# Course statistics
with db.golf as golf:
    stats = golf.get_course_stats('Pebble Beach')
    print(f"Average: {stats['average_score']}")

# All courses
with db.golf as golf:
    courses = golf.get_all_courses()

# Handicap trend
with db.golf as golf:
    trend = golf.get_handicap_trend(limit=20)

# Best rounds
with db.golf as golf:
    best = golf.get_best_rounds(limit=5)
```

### Email

```python
# Recent emails
with db.email as email:
    recent = email.get_recent_emails(days=7)

# Email with key points
with db.email as email:
    msg = email.get_email_with_points(email_id=1)
    print(f"From: {msg['sender']}")
    print(f"Points: {', '.join(msg['key_points'])}")

# Search emails
with db.email as email:
    results = email.search_emails('golf')
```

### Twitter

```python
# Top opportunities
with db.twitter as twitter:
    top = twitter.get_top_opportunities(min_score=70)

# By type
with db.twitter as twitter:
    coaching = twitter.get_by_type('coaching')

# High engagement (follower count)
with db.twitter as twitter:
    influencers = twitter.get_high_engagement(min_followers=10000)

# With types and reasons
with db.twitter as twitter:
    tweet = twitter.get_with_types('tweet_12345')
    print(f"Types: {tweet['all_types']}")
    print(f"Reasons: {tweet['reasons']}")
```

### Analytics

```python
# Recent posts
with db.analytics as analytics:
    posts = analytics.get_recent_posts(days=30)

# Top posts by engagement
with db.analytics as analytics:
    top = analytics.get_top_posts(limit=10)

# Conversions
with db.analytics as analytics:
    conversions = analytics.get_conversions(days=90)

# Revenue by source
with db.analytics as analytics:
    revenue = analytics.get_revenue_by_source()
    for source in revenue:
        print(f"{source['source']}: ${source['total_revenue']:.2f}")

# Total revenue
with db.analytics as analytics:
    total = analytics.get_total_revenue()
    last_30_days = analytics.get_total_revenue(days=30)

# Source performance
with db.analytics as analytics:
    performance = analytics.get_source_performance()
    for source in performance:
        print(f"{source['source']}: {source['conversion_rate']:.1f}% conversion")
```

### Decisions

```python
# Recent decisions
with db.decisions as decisions:
    recent = decisions.get_recent_decisions(days=30)

# By category
with db.decisions as decisions:
    dev_decisions = decisions.get_by_category('development')

# With lessons
with db.decisions as decisions:
    decision = decisions.get_with_lessons(decision_id=1)
    print(f"Decision: {decision['decision']}")
    print(f"Lessons: {', '.join(decision['lessons'])}")

# Hour patterns
with db.decisions as decisions:
    by_hour = decisions.get_by_hour_pattern()

# Day patterns
with db.decisions as decisions:
    by_day = decisions.get_by_day_pattern()
```

## üß™ Testing

### Run All Tests

```bash
cd ~/clawd/database
python3 test_database.py
```

### Test Coverage

- ‚úÖ 30+ test cases
- ‚úÖ All tables tested
- ‚úÖ All query methods tested
- ‚úÖ Performance validation (< 10ms)
- ‚úÖ Data integrity checks
- ‚úÖ Index effectiveness

### Example Test Output

```
test_top_opportunities ... ok (2.3ms)
test_get_by_source ... ok (1.8ms)
test_search ... ok (3.1ms)
test_recent_workouts ... ok (4.2ms)
test_daily_nutrition ... ok (2.1ms)
...

======================================================================
TEST SUMMARY
======================================================================
Tests run: 32
Successes: 32
Failures: 0
Errors: 0
======================================================================
```

## üîÑ Backward Compatibility

### JSON Files Remain Intact

The migration **does not delete** JSON files. They remain in place for:

1. **Rollback capability**: If needed, revert to JSON
2. **External tools**: Other scripts can still read JSON
3. **Verification**: Compare SQLite data with JSON
4. **Gradual migration**: Migrate tools one at a time

### Reading from Both Sources

```python
# Read from SQLite (recommended)
from database.query_library import Database
db = Database()
with db.opportunities as opp:
    opportunities = opp.get_top_opportunities()

# Read from JSON (still works)
import json
with open('revenue/opportunities.json') as f:
    data = json.load(f)
    opportunities = data['inventory']['opportunities']
```

## üìà Performance Benchmarks

All queries optimized for < 10ms execution:

| Query Type | Table | Avg Time | Records |
|------------|-------|----------|---------|
| Top opportunities | opportunities | 1.8ms | 100 |
| Recent workouts | workouts + lifts | 4.2ms | 50 |
| Daily nutrition | food_logs | 2.1ms | 20 |
| Golf rounds | golf_rounds | 1.5ms | 50 |
| Email search | email_summaries | 3.1ms | 200 |
| Twitter top | twitter_opportunities | 2.3ms | 150 |
| Revenue by source | conversions | 2.8ms | 50 |
| Decision patterns | decisions | 3.5ms | 100 |

### Optimization Strategies

1. **Strategic Indexes**: Composite indexes for common query patterns
2. **Minimal Joins**: Denormalized data where appropriate
3. **Selective Queries**: Only fetch needed columns
4. **Generated Columns**: Pre-calculated values (e.g., conversion_rate)
5. **Query Profiling**: Tested with EXPLAIN QUERY PLAN

## üîß Schema Documentation

### Complete Schema

See `schema.sql` for the complete schema with:
- Table definitions
- Indexes
- Foreign keys
- Generated columns
- Default values
- Constraints

### Key Design Decisions

1. **Text IDs for External References**: opportunities, twitter_opportunities
2. **Auto-increment for Internal**: workouts, lifts, decisions
3. **ISO 8601 Timestamps**: All dates in ISO format for sorting
4. **Soft Deletes**: Status fields instead of hard deletes
5. **Audit Columns**: created_at, updated_at for tracking

### Adding New Tables

```sql
-- 1. Create table with proper constraints
CREATE TABLE IF NOT EXISTS new_table (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    data TEXT NOT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);

-- 2. Add strategic indexes
CREATE INDEX IF NOT EXISTS idx_new_table_data ON new_table(data);

-- 3. Update migration script (migrate.py)
def migrate_new_data(self, file_path):
    # Add migration logic
    pass

-- 4. Add query class (query_library.py)
class NewTableQueries(DatabaseQuery):
    def get_data(self):
        # Add queries
        pass

-- 5. Add tests (test_database.py)
class TestNewTable(TestDatabase):
    def test_get_data(self):
        # Add tests
        pass
```

## üêõ Troubleshooting

### Migration Fails

```bash
# Check prerequisites
python3 --version  # Should be 3.7+
sqlite3 --version  # Should be 3.24+

# Run in dry-run mode
python3 migrate.py --dry-run

# Check specific JSON file
python3 -m json.tool < revenue/opportunities.json

# Verbose error output
python3 migrate.py --backup --force 2>&1 | tee migration.log
```

### Performance Issues

```bash
# Analyze query performance
sqlite3 data.db "EXPLAIN QUERY PLAN SELECT * FROM opportunities WHERE score > 70"

# Check index usage
sqlite3 data.db ".schema opportunities"

# Database statistics
sqlite3 data.db "ANALYZE; SELECT * FROM sqlite_stat1;"
```

### Data Verification

```python
# Compare counts
import json
from database.query_library import Database

# Count in SQLite
db = Database()
health = db.health_check()
print(f"SQLite opportunities: {health['opportunities']}")

# Count in JSON
with open('revenue/opportunities.json') as f:
    data = json.load(f)
    print(f"JSON opportunities: {len(data['inventory']['opportunities'])}")
```

## üìù Maintenance

### Regular Backups

```bash
# Backup database
sqlite3 data.db ".backup backups/data_$(date +%Y%m%d).db"

# Backup with compression
sqlite3 data.db ".backup backups/data_$(date +%Y%m%d).db" && \
    gzip backups/data_$(date +%Y%m%d).db
```

### Vacuum Database

```bash
# Reclaim space and optimize
sqlite3 data.db "VACUUM;"

# Analyze for query optimization
sqlite3 data.db "ANALYZE;"
```

### Schema Updates

```bash
# Add column
sqlite3 data.db "ALTER TABLE opportunities ADD COLUMN priority TEXT DEFAULT 'normal';"

# Add index
sqlite3 data.db "CREATE INDEX idx_opportunities_priority ON opportunities(priority);"

# Update migration metadata
sqlite3 data.db "UPDATE migration_metadata SET value='1.1' WHERE key='schema_version';"
```

## üéì Best Practices

### Always Use Context Managers

```python
# ‚úÖ Good: Auto-closes connection
with db.opportunities as opp:
    results = opp.get_top_opportunities()

# ‚ùå Bad: Manual connection management
opp = OpportunityQueries()
opp.conn = sqlite3.connect(DB_PATH)
results = opp.get_top_opportunities()
opp.conn.close()  # Easy to forget!
```

### Use Prepared Statements

```python
# ‚úÖ Good: Prevents SQL injection
cursor.execute('SELECT * FROM opportunities WHERE source = ?', (source,))

# ‚ùå Bad: SQL injection risk
cursor.execute(f'SELECT * FROM opportunities WHERE source = "{source}"')
```

### Limit Query Results

```python
# ‚úÖ Good: Limit results
results = opp.get_top_opportunities(limit=100)

# ‚ùå Bad: Fetch everything
cursor.execute('SELECT * FROM opportunities')
results = cursor.fetchall()  # Could be millions of rows!
```

### Use Transactions for Bulk Inserts

```python
# ‚úÖ Good: Single transaction
conn.execute('BEGIN')
for item in items:
    conn.execute('INSERT INTO table ...', item)
conn.commit()

# ‚ùå Bad: Transaction per insert (slow)
for item in items:
    conn.execute('INSERT INTO table ...', item)
    conn.commit()
```

## üìÑ License

Part of the Clawd workspace. MIT License.

## ü§ù Contributing

1. Update schema in `schema.sql`
2. Update migration in `migrate.py`
3. Add queries in `query_library.py`
4. Add tests in `test_database.py`
5. Update this README
6. Run tests: `python3 test_database.py`

## üìû Support

Issues? Questions?
- Check this README
- Review test cases in `test_database.py`
- Examine example queries in `query_library.py`
- Run dry-run: `python3 migrate.py --dry-run`
